---
title: "Complete IF parameter estimation with POMP"
output:
  html_document:
    df_print: paged
---

Following the example of the outbreak of influenza in a boarding school for boys in England:

https://kingaa.github.io/short-course/stochsim/stochsim.html#the-boarding-school-flu-outbreak
https://kingaa.github.io/clim-dis/mif/mif.html#applying-if2-to-the-boarding-school-influenza-outbreak

```{r}
library(doRNG)
library(foreach)
library(doParallel)

# tidyr has to be imported before magrittr so extract() from the latter is used
library(tidyr)

library(dplyr)
library(plyr)
library(reshape2)
library(magrittr)

library(ggplot2)
theme_set(theme_bw())

library(pomp)
stopifnot(packageVersion("pomp")>="2")
```

```{r}
read.table("http://kingaa.github.io/short-course/stochsim/bsflu_data.txt") -> case_data
max_day <- max(case_data$day)
ggplot(data=subset(case_data, day <= max_day), aes(x=day, y=B, group=1)) + geom_line()
```

```{r}
# Parameters

main_folder <- "."
output_folder <- file.path(main_folder, "boarding") 
if (!dir.exists(output_folder)) dir.create(output_folder)
cooking_folder <- file.path(main_folder, "boarding", "bake")
if (!dir.exists(cooking_folder)) dir.create(cooking_folder)
plotting_folder <- file.path(main_folder, "boarding", "plots")
if (!dir.exists(plotting_folder)) dir.create(plotting_folder)
code_folder <- file.path(main_folder, "boarding", "code")
if (!dir.exists(code_folder)) dir.create(code_folder)

pop_size <- 762
inf0 <- 1

time_step <- 1/12

num_sims <- 100

free_params <- c("Beta", "Gamma", "Rho")
free_param_box <- rbind(
  Beta = c(0.1, 5),
  Gamma = c(0, 3),
  Rho = c(0.5, 1)
)

fixed_params <- c("N", "S0", "I0", "R0", "Sigma")
fixed_param_values <- c(N = pop_size, S0 = pop_size - inf0, I0 = inf0, R0 = 0, Sigma = 0.3324675)

all_param_names <- c(free_params, fixed_params)

# Random seeds, keep unchanged to ensure reproducibilty of results
test_mle_seed <- 0998468235L
full_mle_seed <- 0998468235L
global_search_seed <- 290860873
test_sim_seed <- 157999
full_sim_seed <- 157999
mcap_plik_seed <- 290860873
```

```{r}
# Csnippets defining the SIR model

# pomp C API:
# https://kingaa.github.io/pomp/vignettes/C_API.html

sir_step <- Csnippet("
  double dSI = rbinom(S, 1 - exp(-Beta*I/N*dt));
  double dIR = rbinom(I, 1 - exp(-Gamma*dt));
  double dRR2 = rbinom(R, 1 - exp(-Sigma*dt));

  S -= dSI;
  I += dSI - dIR;
  R += dIR - dRR2;
")

sir_init <- Csnippet("
  S = nearbyint(S0);
  I = nearbyint(I0);
  R = nearbyint(R0);
")

rmeas <- Csnippet("
  B = rpois(Rho * R + tol());
")

dmeas <- Csnippet("
  lik = dpois(B, Rho * R + tol(), give_log);
")

extra <- Csnippet("
double tol() { 
  static double val = 1e-6; 
  return val;
} 
")
```

```{r}
# The documentation of the pomp object
# https://kingaa.github.io/pomp/manual/pomp.html

case_data %>% 
  pomp(t0 = case_data$day[1],
       time = "day",
       rprocess = euler(sir_step, delta.t=time_step),
       rinit = sir_init,
       rmeasure = rmeas,
       dmeasure = dmeas,
       globals = extra,
       cdir = code_folder,
       statenames = c("S", "I", "R"),
       partrans = parameter_trans(
         log = c("Beta", "Gamma", "Sigma"),
         logit = c("Rho")),
       paramnames = c(free_params, fixed_params)
  ) -> model

plot(model, main="")
```

```{r}
# IF parameters, see more details in the manual and tutorial:
# https://kingaa.github.io/pomp/manual/mif2.html
# https://kingaa.github.io/sbied/mif/mif.html#choosing-the-algorithmic-settings-for-if2

num_test_runs <- 10

num_guesses <- 100       # Number of starting points for the parameter guesses
num_filter_iter <- 50    # Number of filtering iterations to perform
num_particles <- 2000    # Number of particles to use in filtering.
num_replicates <- 10     # Number of replicated particle filters at each point estimate

perturb_sizes <- list(Beta=0.02, Gamma=0.02, Rho=0.02)
cool_frac <- 0.5
cool_type <- "geometric"

# Variables to use in the scatterplot matrix showing the result of the IF search
pair_vars <- ~loglik+Beta+Gamma+Rho
```

```{r}
# Test run from single starting point in parameter space and no replicates

registerDoParallel()
#set.seed(test_mle_seed, kind="L'Ecuyer")

guess <- apply(free_param_box, 1, function(x) runif(1, x[1], x[2]))

bake(file=file.path(cooking_folder, "box_search_local.rds"), {
  foreach(i=1:num_test_runs,
          .packages='pomp', .combine=c, .options.multicore=list(set.seed=TRUE)
  ) %dopar%  
  {
    mif2(
      model,
      params=c(guess, fixed_param_values),
      Np=num_particles,
      Nmif=num_filter_iter,
      cooling.type=cool_type,
      cooling.fraction.50=cool_frac,
      rw.sd=do.call(rw.sd, perturb_sizes)
    )
  }
}) -> mifs_test

ggplot(data=melt(traces(mifs_test)),
       aes(x=iteration, y=value, group=L1, color=factor(L1))) +
  geom_line() +
  guides(color=FALSE) +
  facet_wrap(~variable, scales="free_y") +
  theme_bw()

ggsave(file.path(plotting_folder, "1-mle_local_search.pdf"))
```

```{r}
# Full MLE with multiple starting points for the free parameters

registerDoParallel()
set.seed(full_mle_seed, kind="L'Ecuyer")

stew(file=file.path(cooking_folder, "box_search_global.rda"), {
  param_guesses <- as.data.frame(apply(free_param_box, 1, function(x) runif(num_guesses, x[1], x[2])))
  
  workers <- getDoParWorkers()
  systime <- system.time({
    res_global <- foreach(guess=iter(param_guesses,"row"), 
                         .packages='pomp', .combine=rbind, .options.multicore=list(set.seed=TRUE)
    ) %dopar% {
      mifs_local <- mif2(
        model,
        params=c(unlist(guess), fixed_param_values),
        Np=num_particles,
        Nmif=num_filter_iter,
        cooling.type=cool_type,
        cooling.fraction.50=cool_frac,
        rw.sd=do.call(rw.sd, perturb_sizes)
        )
      ll <- logmeanexp(replicate(num_replicates, logLik(pfilter(mifs_local, Np=num_particles))), se = TRUE)
      data.frame(as.list(coef(mifs_local)), loglik=ll[1], loglik.se=ll[2])
    }
  })
}, seed=global_search_seed, kind="L'Ecuyer")

res_global <- as.data.frame(res_global)

all <- ldply(list(guess=param_guesses, result=subset(res_global, loglik > max(loglik)-50)), .id="type")

pairs(pair_vars, data=all, col=ifelse(all$type=="guess", grey(0.5), "red"), pch=16)
dev.copy(pdf, file.path(plotting_folder, "2-mle_global_search.pdf"))
dev.off()
```

```{r}
# Getting best parameters

mle_params <- arrange(rbind(res_global), -loglik)
write.csv(mle_params, file=file.path(output_folder, "param_mle_global_search.csv"), row.names=FALSE, na="")

log_idx <- length(mle_params) - 1
mle_global <- mle_params[which.max( mle_params[,log_idx] ), ]
mle_global %>% extract(all_param_names) %>% unlist() -> theta
write.csv(theta, file=file.path(output_folder, "param_point_estimates.csv"), row.names=TRUE, na="")
theta
```

```{r}
# Running simulations using the MLE parameters

#set.seed(test_sim_seed)

model  %>%
  simulate(params=theta, nsim=9, format="data.frame", include.data=TRUE) %>%
  ggplot(aes(x=day, y=B, group=.id, color=(.id=="data"))) +
  guides(color=FALSE) +
  geom_line() + facet_wrap(~.id, ncol=2)

ggsave(file.path(plotting_folder, "3-simulations.pdf"))
```

```{r}
# Computing a large number of simulations

set.seed(full_sim_seed)

# Getting the median cumulative curve for the simulations
model %>% 
  simulate(params=theta, nsim=num_sims, format = "data.frame", include.data=TRUE) -> sim_data
```

```{r}
# Compare the observed data with the simulated data between the 5th and 95th percentiles

sim_data %>%
  select(day, .id, B) %>%
  mutate(data=.id=="data") %>%
  ddply(~day+data, plyr::summarize,
    p=c(0.05, 0.5, 0.95), q=quantile(B, prob=p, names=FALSE)) %>%
  mutate(
    p=mapvalues(p, from=c(0.05, 0.5, 0.95), to=c("lo", "med", "hi")),
    data=mapvalues(data, from=c(TRUE, FALSE), to=c("data", "simulation"))
  ) %>%
  spread(p, q) %>%
  ggplot(aes(x=day, y=med, color=data, fill=data, ymin=lo, ymax=hi)) +
         geom_ribbon(alpha=0.2)

ggsave(file.path(plotting_folder, "4-simulated_percentiles.pdf"))
```

```{r}
# Some utility functions to calculate cumulative case numbers

cumulative_curve <- function(dat, len) {
  total_sum <- 0
  daily_sum <- c()
  for (i in 1:len) {
    total_sum <- total_sum + dat$B[i]
    daily_sum <- c(daily_sum, total_sum)
  }
  return(daily_sum)
}  
  
median_simulation <- function(sdat, n) {
  all_totals <- c()
  for (i in 1:n) {
    sim <- subset(sdat, .id == i)
    tot <- sum(sim$B)
    all_totals <- c(all_totals, tot)
  }

  # Taking the median
  n2 <- 0.5 * n
  median_idx <- order(all_totals)[n2]
  median_sim <- subset(sdat, .id == median_idx)
  
  return(median_sim)
}
```

```{r}
# Comparing cumulative actual data with real data

med_sim <- median_simulation(sim_data, num_sims)
csim <- cumulative_curve(med_sim, max_day)

# Getting the cumulative curve for observed data
cobs <- cumulative_curve(case_data, max_day)

df <- data.frame('day' = seq(1, max_day), 
                  'obs_data' = cobs,
                  'sim_data' = csim)

ggplot(df, aes(day)) + 
  geom_line(aes(y = obs_data, colour = "Real Data")) + 
  geom_line(aes(y = sim_data, colour = "Simulated")) +
  ylab('Cumulative Number of Cases')

ggsave(file.path(plotting_folder, "4-cumulative_cases.pdf"))
```

CALCULATION OF THE CONFIDENCE INTERVAL FOR THE PARAMETERS (OPTIONAL)

Note that calculating the CIs for all the parameters in the model can be quite time consuming!
The method used here (MCAP, see references below) involves recalculating the likelihood surface 
for a number of fixed values of the parameter of interest.

Parameter estimation tutorial: http://kingaa.github.io/short-course/parest/parest.html
Standard errors for the MLE: https://kingaa.github.io/sbied/pfilter/pfilter.html#standard-errors-for-the-mle
Monte Carlo profile (MCAP) confidence intervals: https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0126

```{r}
# MCAP settings

mcap_confidence <- 0.95  # The desired confidence

# Profile design settings
mcap_profdes_len <- 15   # Number of subdivisions of the parameter range in the profile
mcap_nprof <- 10         # Number of starts per profile point
mcap_replicates <- 5
mcap_num_particles_pfilt <- 2000

# IF settings to generate the likelihood surface for each parameter
mcap_num_particles <- 1000
mcap_num_filter_iter <- 50

mcap_cool_type <- "geometric"
mcap_cool_frac <- 0.5
mcap_cool_frac_lastif <- 0.1

# Lambda closer to 1 increases the curvature of the likelihood approximation
mcap_lambda <- c(Beta=0.75, Gamma=0.75, Rho=0.75)

mcap_ngrid <- c(Beta=1000, Gamma=1000, Rho=1000)
```

```{r}
# Function to calculate the MCAP CIs 

mcap <- function(lp, parameter, confidence, lambda, Ngrid) {
  smooth_fit <- loess(lp ~ parameter, span=lambda)
  parameter_grid <- seq(min(parameter), max(parameter), length.out = Ngrid)
  smoothed_loglik <- predict(smooth_fit, newdata=parameter_grid)
  smooth_arg_max <- parameter_grid[which.max(smoothed_loglik)]
  dist <- abs(parameter - smooth_arg_max)
  included <- dist < sort(dist)[trunc(lambda*length(dist))]
  maxdist <- max(dist[included])
  weight <- rep(0, length(parameter))
  weight[included] <- (1 - (dist[included]/maxdist)^3)^3
  quadratic_fit <- lm(lp ~ a + b, weight=weight,
                      data = data.frame(lp=lp, b=parameter, a=-parameter^2))
  b <- unname(coef(quadratic_fit)["b"] )
  a <- unname(coef(quadratic_fit)["a"] )
  m <- vcov(quadratic_fit)
  
  var_b <- m["b", "b"]
  var_a <- m["a", "a"]
  cov_ab <- m["a", "b"]
  
  se_mc_squared <- (1 / (4 * a^2)) * (var_b - (2 * b/a) * cov_ab + (b^2 / a^2) * var_a)
  se_stat_squared <- 1/(2*a)
  se_total_squared <- se_mc_squared + se_stat_squared
  
  delta <- qchisq(confidence, df=1) * ( a * se_mc_squared + 0.5)
  loglik_diff <- max(smoothed_loglik) - smoothed_loglik
  ci <- range(parameter_grid[loglik_diff < delta])
  list(lp=lp, parameter=parameter, confidence=confidence,
       quadratic_fit=quadratic_fit, quadratic_max=b/(2*a),
       smooth_fit=smooth_fit,
       fit=data.frame(parameter=parameter_grid,
                      smoothed=smoothed_loglik,
                      quadratic=predict(quadratic_fit, list(b = parameter_grid, 
                                                            a = -parameter_grid^2))),
       mle=smooth_arg_max, ci=ci, delta=delta,
       se_stat=sqrt(se_stat_squared), se_mc=sqrt(se_mc_squared), 
       se=sqrt(se_total_squared)
  )
}
```

```{r}
# Generates the model for the profile likelihood calculation

plik <- function(pname, pval0, pval1) {
  registerDoParallel()
  
  bake(file=file.path(cooking_folder, paste(pname, "_mcap.rds", sep="")), {  
    
    desel <- which(names(mle_params) %in% c("loglik", "loglik.se", pname))
    mle_params %>% 
      subset(
        loglik > max(loglik,na.rm=TRUE) - 20,
        select=-desel
      ) %>% 
      melt(id=NULL) %>% 
      daply(~variable, function(x)range(x$value)) -> box
    
    starts <- profileDesign(pname=seq(pval0, pval1, length=mcap_profdes_len),
                            lower=box[,1], upper=box[,2],
                            nprof=mcap_nprof)
    names(starts) <- sub("pname", pname,names(starts))
    
    psizes <- perturb_sizes[names(perturb_sizes) != pname]
    
    foreach(params=iter(starts, "row"),
            .combine=rbind,
            .packages="pomp",
            .options.multicore=list(set.seed=TRUE),
            .options.mpi=list(seed=mcap_plik_seed, chunkSize=1)
    ) %dopar% {
      mf <- mif2(model,
                 params=unlist(params),
                 Np=mcap_num_particles,
                 Nmif=mcap_num_filter_iter,
                 cooling.type=mcap_cool_type,
                 cooling.fraction.50=mcap_cool_frac,
                 rw.sd=do.call(rw.sd, psizes)
      )
      mf <- mif2(mf, 
                 Np=mcap_num_particles,
                 Nmif=mcap_num_filter_iter,
                 cooling.fraction.50=mcap_cool_frac_lastif)
      ll <- logmeanexp(replicate(mcap_replicates, 
                                 logLik(pfilter(mf, Np=mcap_num_particles_pfilt))), se=TRUE)
      data.frame(as.list(coef(mf)), loglik=ll[1], loglik.se=ll[2])
    }
  }) -> pmodel
  
  return(pmodel)
}
```

```{r}
# Iterate over all the free parameters in the model to calculate their CIs... may take a while!

par_names <- row.names(free_param_box)
for (i in 1:nrow(free_param_box)) {
  name <- par_names[i]  
  print(sprintf("Calculating CI for %s...", name))
  
  row <- free_param_box[i,]
  mdl <- plik(pname=name, pval0=row[1], pval1=row[2])
  
  par_range <- seq(row[1], row[2], length=mcap_profdes_len)
  log_likelihoods <- c()
  for (val in par_range) {
    likelihoods <- subset(mdl, abs(mdl[[name]]-val)<1)$loglik
    if (length(likelihoods) == 0) next
    log_likelihoods <- c(log_likelihoods, max(likelihoods))
  }
  
  x <- mcap(log_likelihoods, par_range, mcap_confidence, mcap_lambda[[i]], mcap_ngrid[[i]])
  if (i == 1) {
    cis <- data.frame("name" = c(name), "x0" = c(x$ci[1]), "x1" = c(x$ci[2]), stringsAsFactors = FALSE)  
  } else {
    cis <- rbind(cis, c(name, x$ci[1], x$ci[2]))  
  }
  print(sprintf("%s %0.2f %0.2f", name, x$ci[1], x$ci[2]))

  ggplot(x$fit, aes(parameter, quadratic)) + geom_line() + 
    geom_vline(xintercept=c(x$ci[1], x$ci[2]), linetype=4, colour='red') +
    geom_point(data = data.frame('parameters'=par_range, 'loglik'=log_likelihoods), 
               aes(parameters, log_likelihoods)) 
  ggsave(file.path(plotting_folder, paste("5-", name, "_ci.pdf", sep="")))
}

write.csv(cis, file=file.path(output_folder, "param_confidence_intervals.csv"), row.names=FALSE, na="")
```



